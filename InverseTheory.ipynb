{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian approaches to parameter inversion\n",
    "\n",
    "This notebook is a short training material with some background on retrieving parameters of interest from satellite data. Within this context, the parameters of interest represent magnitudes of things that we could measure *in situ* and that are of interest as they describe the state of the land surface.\n",
    "\n",
    "We wish to *infer* these magnitudes from indirect measurements of the spectral properties of the surface (e.g. reflectance at different wavelengths in the optical domain) and/or the magnitude of the backscatter in the microwave domain (using Synthetic Aperture Radar, SAR, data).\n",
    "\n",
    "The parameters of interest include:\n",
    "* Leaf area index (LAI): amount of one sided leaf area per unit area of ground $[m^{2}\\cdot m^{2}]$\n",
    "* Leaf pigment concentrations, such as the concentration of chlorophyll, water and senescent vegetation in the leaves\n",
    "* Soil moisture content\n",
    "* Soil roughness\n",
    "\n",
    "## The data sources\n",
    "\n",
    "### Optical data\n",
    "Optical data (e.g. from Sentinel 2 or Landsat sensors) provides measurements of surface reflectance at a number of discrete spectral bands covering the solar reflective domain (from 400 to 2500 nm, which includes the visible part of the spectrum, from around 400 to 700 nm). The optical signal is obscured by clouds and night. We have a number of physical models (based on radiative transfer theory) that relate the surface parameters to the reflectance as seen by the sensor. Mathematically, and denoting the acquisition geometry by $\\Omega, \\Omega'$, the wavelengths of the sensor by $\\lambda_i$ and calling the surface reflectance $\\rho$:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(LAI, C_{ab}, C_{w}, \\dots, \\Omega, \\Omega') = \\rho(\\lambda_i, \\Omega, \\Omega')\n",
    "$$\n",
    "\n",
    "The problem is that we measure the stuff on the right hand side, but want to figure out the parameters in the left hand side, so we want to use the model $\\mathcal{H}$ \"backwards\". This is called the \"inverse problem\".\n",
    "\n",
    "\n",
    "### SAR data\n",
    "\n",
    "SAR data illuminate the land surface with a microwave beam and measure the strength of the returns. This is measurement is called backscatter, and is often measured in different polarisations. In this project, we only consider the Sentinel 1 sensor, which operates with a frequency of around 5 GHz in the so-called C band, and measures backscatter in VV and VH polarisations. So each pixel in a Sentinel 1 image has two \"bands\": VV and VH. In the same way that we have physical models for the optical, we also have them for predicting SAR backscatter in the $pq$ polarisation, $\\sigma^{0}_{pq}$:\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(sm, k, LAI, \\dots, \\theta)=\\sigma^{0}_{pq}(\\theta)\n",
    "$$\n",
    "\n",
    "Again, we are faced with the problem of how to go from the right hand side to the left hand side.\n",
    "\n",
    "It gets even worse: due to the limitations of sensors, there is an extra term on the right hand side, which is observational noise. It is also important to understand that the physical have differential sensitivity to different parameters, so one or two parameters may be have an important effect in the signal, whereas other might not have a measurable impact.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse problems\n",
    "\n",
    "Let's introduce some common notation. The parameters of interest will be stacked in a vector $\\vec{x}$. If we consider a single pixel at a single date, we have that we can have a set of observations of e.g. reflectance and/or backscatter, which we'll stack in a vector $\\vec{y}$. The physical model we'll continue calling $\\mathcal{H}$ if it's nonlinear (the usual case) or $\\mathbf{H}$ if it's linear (not uncommon, but you may be able to locally linearise $\\mathcal{H}$ via e.g. a Taylor series). \n",
    "\n",
    "The *measurement equations*, assuming our observations are contaminated by additive zero-mean Gaussian noise characterised by a covariance matrix $\\mathbf{C}_{obs}^{-1}$ is\n",
    "\n",
    "$$\n",
    "\\mathcal{H}(\\vec{x})=\\vec{y} + \\mathcal{N}(0, \\mathbf{C}_{obs}^{-1}),\n",
    "$$\n",
    "\n",
    "Or, in other words, the difference $\\mathcal{H}(\\vec{x}) - \\vec{y})$ is a [multivariate Gaussian distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Density_function):\n",
    "\n",
    "$$\n",
    "f_{\\mathbf X}(x_1,\\ldots,x_k) = \\frac{\\exp\\left(-\\frac 1 2 ({\\mathbf x}-{\\boldsymbol\\mu})^\\mathrm{T}{\\boldsymbol C}^{-1}_{obs}({\\mathbf x}-{\\boldsymbol\\mu})\\right)}{\\sqrt{(2\\pi)^k|\\boldsymbol C_{obs}|}}.\n",
    "$$\n",
    "Assuming $\\mu=0$, and taking logs to simplify things, and ignoring an unimportant additive term (e.g. the the best solution would be one that minimises\n",
    "\n",
    "$$\n",
    "J_{obs}(\\vec{x}) = \\frac{1}{2}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]^{\\top}\\mathbf{C}_{obs}^{-1}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]\n",
    "$$\n",
    "\n",
    "We can minimise this with a non-linear solver. However, due to noise and limited sensitivity of $\\vec{y}$ to $\\vec{x}$ or parts of it, the solution is usually noisy or not acceptable. A natural way to constrain the solution is to recognise the problem we have solved as a way of maximising the log-likelihood (or minimising the negative log likelihood rather but you get my drift...), and turning it into a Bayesian problem, by extending it to have a parameter prior PDF. We will assume further than the prior PDF is also multivariate Gaussian, with a mean $\\vec{\\mu}_{x}$ and covariance $\\mathbf{C}_{prior}$. This results in $J_{prior}(\\vec{x})$, by similar arguments as above\n",
    "\n",
    "$$\n",
    "J_{prior} = \\frac{1}{2}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right]^{\\top}\\mathbf{C}_{prior}^{-1}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right].\n",
    "$$\n",
    "\n",
    "Combining them into the *a posteriori* pdf, we have a combined cost fuction given by\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\vec{x}) &= J_{obs}(\\vec{x})+   J_{prior}(\\vec{x}) \\\\\n",
    "&= \\frac{1}{2}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right]^{\\top}\\mathbf{C}_{prior}^{-1}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right]+\\frac{1}{2}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]^{\\top}\\mathbf{C}_{obs}^{-1}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If we assume that $\\mathcal{H}$ isn't very non-linear, we can assume that the posterior pdf is also Gaussian, with a mean given by the minimum of $J(\\vec{x})$ and an associated covariance matrix given by the inverse of the Hessian (matrix of second order derivatives) of $J(\\vec{x})$ around the minimum.\n",
    "\n",
    "In essence, this is the what we are after: minimising some cost function, and in order to do that efficiently, exploting Newton or quasi-Newton minimisation methods, that require access to the gradient (Jacobian) of $J(\\vec{x})$, $\\nabla J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical models\n",
    "\n",
    "The parameters that we mentioned above (LAI, soil moisture) tend to vary over time (if it rains, soil moisture tends to increase, LAI varies over the growing season, ...). It is also sensible to assume that some of these parameters change *slowly* (e.g. LAI changes little day by day). Understanding the scale of temporal dynamics of the parameters allows us to combine evidence to strengthen our inference on the parameters. For example: if the LAI is assumed constant over a week, and we have 4 observations within that week, we can combine the incomplete estimates (e.g. solve four individual minimisation problems and combine the results weighted by the uncertainty of each one).\n",
    "\n",
    "Understanding this leads on to strategies to solve problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAR and optical retrieval\n",
    "\n",
    "In KaSKA, Sentinel 2 data are used to retrieve a time series of S2 parameters. We think that we can use these as a prior for the Sentinel 1 inversion. The Sentinel 1 physical model is the water cloud model (WCM), that basically has two contributions to the measured signal: scatter from the canopy plus a contribution from the underlyign soil attenuated by the canopy. The parameters are given in `LMU_all_fields.ipynb`, but\n",
    "$$\n",
    "\\sigma_{pq}^{0} = A\\cdot V_{1}\\left[1 - \\exp\\left(-\\frac{-2B\\cdot V_{2}}{\\cos\\theta}\\right)\\right] + \\exp\\left(-\\frac{-2B\\cdot V_{2}}{\\cos\\theta}\\right)\\cdot\\left(\\sigma_{soil}(pq,sm,k,\\theta)\\right),\n",
    "$$\n",
    "where $A$ and $B$ are some empirical constants, $\\theta$ is the angle of incidence (given by the data product), $\\sigma_{soil}$ is the soil backscatter (usually calculated as a function of polarisation $pq$, angle of incidence, soil moisture and soil roughness), and $V_1$ and $V_2$ describe the canopy optical thickness, and are usually parameterised by LAI.\n",
    "\n",
    "We develop a retrieval strategy that aims to exploit the following information:\n",
    "\n",
    "1. We have a reasonable guess at LAI from the S2 data, so that $V_1=V_2=LAI$\n",
    "2. LAI should evolve smoothly,\n",
    "3. $A$ and $B$ parameters relate to canopy structure and should either vary smoothly or could be fixed for a pixel in time. They're given per polarisation, eg $A_{VV}$ and $A_{VH}$\n",
    "4. A similar argument holds for $k$ (soil roughness)\n",
    "5. $sm$ (soil moisture) is probably not smoothly varying in time\n",
    "6. We can also simplify $\\sigma_{soil}$ to be $C\\cdot sm$, where $C$ is polarisation dependent and constant over time, and\n",
    "7. have an extra term $D$ to remove biases etc.\n",
    "\n",
    "This leads us to the following $\\vec{x}=\\left[A_{VV}, B_{VV}, C_{VV}, A_{VH}, B_{VH}, C_{VH}, D_{VH}, LAI_{1},\\dots,LAI_{N_Obs},sm_{1}, \\dots,sm_{N_Obs}\\right]^{\\top}$, where we are solving for all these parameters using data over a temporal window from $t=1$ to $t=N_{Obs}$. We can put prior PDFs on parameters, but we can encode smoothness by adding a smoothness constraint:\n",
    "\n",
    "$$\n",
    "J_{smooth}(\\vec{x})=\\frac{1}{2}\\gamma\\displaystyle{\\sum_{k=1}^{N_{Obs}-1}\\left[x_{k}-x_{k+1}\\right]^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the light of the above, it is interesting to consider the cost function that we use in the example notebook:\n",
    "\n",
    "```python\n",
    "def cost_function(x, svh, svv, theta, gamma, prior_mean, prior_unc, unc=0.8):\n",
    "    \"\"\"A combined cost function that calls the prior, fit to the observations\n",
    "    \"\"\"\n",
    "    # Fit to the observations\n",
    "    cost1, dcost1 = cost_obs(x, svh, svv, theta, unc=unc)\n",
    "    # Fit to the prior\n",
    "    cost2, dcost2 = cost_prior(x, svh, svv, theta, prior_mean, prior_unc)\n",
    "    # Smooth evolution of LAI\n",
    "    n_obs = len(svv)\n",
    "    lai = x[(6 + n_obs) :]\n",
    "    cost3, dcost3 = cost_smooth(lai, gamma)\n",
    "    tmp = np.zeros_like(dcost1)\n",
    "    tmp[(7 + n_obs) : -1] = dcost3\n",
    "    return cost1 + cost2 + cost3, dcost1 + dcost2 + tmp\n",
    "```\n",
    "\n",
    "I have aimed to transplant the $J(\\vec{x}) = J_{obs}(\\vec{x})+   J_{prior}(\\vec{x}) + J_{smooth}(\\vec{x})$ concept here. Things to note are that there's only smoothing of LAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clever clogs extensions\n",
    "\n",
    "The problem we're solving requires an expensive non-linear minimisation only because $\\mathcal{H}$ is non-linear. We can encode the smoother as a first order differences matrix $\\Delta$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\vec{x}) &= J_{obs}(\\vec{x})+   J_{prior}(\\vec{x}) + J_{smooth}(\\vec{x}) \\\\\n",
    "&= \\frac{1}{2}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right]^{\\top}\\mathbf{C}_{prior}^{-1}\\left[\\vec{x} - \\vec{\\mu}_{x}\\right]\\\\\n",
    "&+\\frac{1}{2}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]^{\\top}\\mathbf{C}_{obs}^{-1}\\left[\\mathcal{H}(\\vec{x}) - \\vec{y}\\right]\\\\\n",
    "&+\\frac{1}{2}\\gamma\\vec{x}^{\\top}\\Delta^{\\top}\\Delta\\vec{x}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Assume that we can linearise $\\mathcal{H}$ around our e.g. prior mean, then the whole problem can be written linearly. A first order Taylor approximation to $\\mathcal{H}$:\n",
    "$$\n",
    "\\mathcal{H}(\\vec{x}_0 + \\vec{x}) \\sim \\mathcal{H}(\\vec{x}_0) + \\frac{\\overbrace{\\partial \\mathcal{H}(\\vec{x}_0)}^{\\mathbf{H}'(\\vec{x})}}{\\partial \\vec{x}}\\cdot \\left(\\vec{x}_0 - \\vec{x}\\right)\n",
    "$$\n",
    "           \n",
    "           \n",
    "Bayesian inference problem necessitates calculating log-posterior as\n",
    "$$\n",
    "\\begin{aligned}\n",
    " \\left[\\mathcal{H}_0 + \\mathbf{H}'(\\vec{x} - \\vec{x}_0  ) - \\vec{y} \\right]^{\\top}\\mathbf{C}_{obs}^{-1}\\left[\\mathcal{H}_0 + \\mathbf{H}'(\\vec{x}_0 - \\vec{x}) - \\vec{y} \\right]& \\\\\n",
    "+& \\left[\\vec{x} - \\vec{\\mu}_{x} \\right]^{\\top}\\mathbf{C}^{-1}_{prior}\\left[\\vec{x} - \\vec{\\mu}_{x} \\right]\\\\\n",
    "+& \\gamma\\cdot \\Delta^{\\top}\\Delta\\vec{x}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Re-arranging terms and $\\vec{r}=\\vec{y} - \\mathcal{H}_0 + \\mathbf{H}'\\vec{x}_0$\n",
    "$$\n",
    "\\overbrace{\\left[\\mathbf{H}'^{\\top}\\mathbf{C}_{obs}^{-1}\\mathbf{H}' + \\mathbf{C}_{prior}^{-1} + \\Delta^{\\top}\\Delta \\right]}^{\\mathbf{A}}\\vec{x} =  \\underbrace{\\mathbf{H}'^{\\top}\\mathbf{C}_{obs}^{-1}\\vec{r} +  \\mathbf{C}_{prior}^{-1}\\vec{\\mu}_{x}}_{\\vec{b}}\n",
    "$$\n",
    "\n",
    "Now that looks pretty horrible, but this is a linear problem, where the $\\mathbf{A}$ matrix is block diagonal (efficient to invert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
